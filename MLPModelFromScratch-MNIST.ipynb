{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64006dc-cc29-4f5e-915e-fbde5675b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm import tqdm as tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58731f54-4cb5-4317-9862-a9b20f8fe11e",
   "metadata": {},
   "source": [
    "<h1>Weight Initialization Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf170d-8a38-4163-b19f-d3e752f1c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zeros(shape):\n",
    "    return np.zeros(shape)\n",
    "def xavier_normal(shape):\n",
    "    fan_out, fan_in = shape\n",
    "    std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "    return np.random.normal(0.0,std,size = shape)\n",
    "def he(shape):\n",
    "    fan_out, fan_in = shape\n",
    "    std = np.sqrt(2/fan_in)\n",
    "    return np.random.normal(0.0, std, size = shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ee9c4-28d9-4cef-800f-e613ebaebf6a",
   "metadata": {},
   "source": [
    "<h1>Layers Classes for layers</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d627a9d-72b5-496f-bc1a-620c3cfcf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers() : \n",
    "    def forward(self,X,training = True):\n",
    "        raise NotImplementedError(\"forward() method is not implemented, but still called !\")\n",
    "    def backward(self):\n",
    "        raise NotImplementedError(\"forward() method is not implemented, but still called !\")\n",
    "        \n",
    "        \n",
    "class Linear(Layers):\n",
    "    def __init__(self, in_dim, out_dim, weight_decay = 0.0, weight_init = he, bias_init = zeros):\n",
    "        #define the @weight_init function before using it, it is not defined by default\n",
    "        self.W = weight_init((out_dim,in_dim))\n",
    "        self.b = bias_init((out_dim,))\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def forward(self,X,training = True):\n",
    "        if training:\n",
    "            self.x = X\n",
    "        return X @ self.W.T + self.b\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        self.dW = dout.T @ self.x\n",
    "        if self.weight_decay > 0:\n",
    "            self.dW += 0.5 * self.weight_decay * self.W\n",
    "        self.db = np.sum(dout,axis = 0)\n",
    "        \n",
    "        dx = dout @ self.W\n",
    "        return dx\n",
    "    \n",
    "class ReLU(Layers):\n",
    "    def forward(self,X,training = True):\n",
    "        if training:\n",
    "            self.mask = X > 0\n",
    "        return X*(X > 0)\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        return dout*self.mask\n",
    "\n",
    "class Softmax(Layers):\n",
    "    def forward(self, x):\n",
    "        x_shifted = x - np.max(x, axis = 1, keepdims = True)\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        self.probs = exp_x/np.sum(x_shifted, axis = 1, keepdims = True)\n",
    "        return self.probs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aa331d-7a4b-4968-897e-a33a909dac2a",
   "metadata": {},
   "source": [
    "<h1>Loss function && Accuracy </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc9392-e378-498a-ba51-4b63ed7ab19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def forward(self):\n",
    "        raise NotImplementedError(\"Loss function not implemented..!\")\n",
    "    def backward(self):\n",
    "        raise NotImplementedError(\"Loss function not implemented..!\")\n",
    "        \n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    \n",
    "        def forward(self, X, y):\n",
    "            \n",
    "            #softmax\n",
    "            x_shifted = X - np.max(X , axis = 1, keepdims = True)\n",
    "            self.y = y\n",
    "            exp_x = np.exp(x_shifted)\n",
    "            self.probs = exp_x/np.sum(exp_x, axis = 1, keepdims = True)\n",
    "            \n",
    "            #CCE loss\n",
    "            log_probs = -np.log(self.probs[np.arange(self.probs.shape[0]), y])#self.probs.shape[0] = batch size\n",
    "            loss = np.mean(log_probs)\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        def backward(self):\n",
    "            \n",
    "            dx = self.probs.copy()\n",
    "            dx[np.arange(self.probs.shape[0]), self.y] -= 1\n",
    "            dx /= self.probs.shape[0]\n",
    "            return dx\n",
    "        \n",
    "def compute_l2_loss(model):\n",
    "    \"\"\"\n",
    "    Computes L2 regularization loss over all trainable weights in the model.\n",
    "\n",
    "    Returns:\n",
    "        l2_loss (float): scalar L2 penalty\n",
    "    \"\"\"\n",
    "    l2_loss = 0.0\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'W') and hasattr(layer, 'weight_decay'):\n",
    "            if layer.weight_decay > 0:\n",
    "                l2_loss += 0.5 * layer.weight_decay * np.sum(layer.W ** 2)\n",
    "    return l2_loss\n",
    "\n",
    "def accuracy_score(y_true, preds):\n",
    "    y_pred = np.argmax(preds, axis = 1)\n",
    "    return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e29f9-d4ec-48d6-925e-99b2b8637af3",
   "metadata": {},
   "source": [
    "<h1>Optimizer and Early Stopping</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d98fa6-106c-4dcf-8d59-b7dd9b2c84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizers():\n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "class Adam(Optimizers):\n",
    "    def __init__(self, layers, lr = 0.001, beta1 =0.95, beta2 = 0.95, eps = 1e-08):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.velocity = {}\n",
    "        self.cache = {}\n",
    "        self.t = 0\n",
    "        \n",
    "        self.params = []\n",
    "        for layer in layers:\n",
    "            for name in ['W', 'b', 'gamma', 'beta']:\n",
    "                if hasattr(layer, name):\n",
    "                    self.params.append((layer, name))\n",
    "                    params = getattr(layer, name)\n",
    "                    self.cache[(layer, name)] = np.zeros_like(params)\n",
    "                    self.velocity[(layer, name)] = np.zeros_like(params)\n",
    "                    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        \n",
    "        for layer, name in self.params:\n",
    "            grad = getattr(layer, 'd'+name)\n",
    "            velocity = self.velocity[(layer, name)]\n",
    "            cache = self.cache[(layer, name )]\n",
    "            \n",
    "            #velocity update\n",
    "            velocity[:] = self.beta1 * velocity + (1 - self.beta1) * grad\n",
    "            cache[:] = self.beta2 * cache + (1 - self.beta2) * (grad ** 2)\n",
    "            \n",
    "            #bias correction\n",
    "            v_hat = velocity / (1 - self.beta1 ** self.t)\n",
    "            c_hat = cache / (1 - self.beta2 ** self.t)\n",
    "            setattr(layer, name, getattr(layer, name) - self.lr * v_hat / (np.sqrt(c_hat) + self.eps))\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, restore_best_params=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.stop = False\n",
    "        self.best_params = None\n",
    "        self.restore_best_params = restore_best_params\n",
    "        self.best_epoch = None\n",
    "        self.epoch_counter = 0\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        self.epoch_counter += 1\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = self.epoch_counter\n",
    "            self.counter = 0\n",
    "            if self.restore_best_params:\n",
    "                self.best_params = self._copy_params(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.stop = True\n",
    "\n",
    "    def restore_best(self, model):\n",
    "        if self.best_params is not None:\n",
    "            self._set_params(model, self.best_params)\n",
    "\n",
    "    def _copy_params(self, model):\n",
    "        params = []\n",
    "        for layer in model.layers:\n",
    "            layer_params = {}\n",
    "            for name in ['W', 'b', 'gamma', 'beta']:\n",
    "                if hasattr(layer, name):\n",
    "                    layer_params[name] = getattr(layer, name).copy()\n",
    "            params.append(layer_params)\n",
    "        return params\n",
    "\n",
    "    def _set_params(self, model, params):\n",
    "        for layer, layer_params in zip(model.layers, params):\n",
    "            for name, value in layer_params.items():\n",
    "                setattr(layer, name, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee475b",
   "metadata": {},
   "source": [
    "<h1> Model Architecture (Sequential Container)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential: \n",
    "    def __init__(self,layers): \n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, x, training = True): \n",
    "        for layer in self.layers : \n",
    "            x = layer.forward(x, training) \n",
    "        return x \n",
    "    \n",
    "    def backward(self, dout): \n",
    "        for layer in reversed(self.layers) : \n",
    "            dout = layer.backward(dout) \n",
    "        return dout \n",
    "    \n",
    "    def predict(self, x): \n",
    "        logits = self.forward(x, training = False) \n",
    "        return np.argmax(logits, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8c2e2-2c11-4801-8103-14d8af35b7a5",
   "metadata": {},
   "source": [
    "<h1>Data Loading,shuffling and Train-Test-Split</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc9b8d-6bcd-4ad7-a77d-9193aaf83572",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Lambda(lambda x: x.view(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7891f72-9fb2-469f-9dae-21059037ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root=\"./data\", train = True, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0784a08-80c7-49c9-97ca-f3d7264665fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f311e1-741d-444c-8ac4-f337ddb1101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_dataset[2]\n",
    "img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e05e3-7eba-4e2a-97bb-ae362f759e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b85ec-abc3-45f6-be1f-a22baf5814d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47dbe5c-ae2b-4754-bf6c-929c19f21087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))\n",
    "print(img.shape)\n",
    "print(img.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5a63e-63b8-4731-b194-c84e2dbce05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29870ab5-ae79-423f-a21c-3601df411976",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for img, label in train_dataset:\n",
    "    X_list.append(img.numpy())\n",
    "    y_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca14e1-d7f3-45de-a357-934692b680f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_list) #input data\n",
    "y = np.array(y_list) #label/targets\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae17bd8-c98e-42ff-a7c7-be1a8dd67805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_size = None, test_size = 0.2, random_state = None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    N = len(X)\n",
    "\n",
    "    # decide sizes\n",
    "    if train_size is None and test_size is None:\n",
    "        raise ValueError(\"At least one of train_size or test_size must be specified\")\n",
    "\n",
    "    if test_size is None:\n",
    "        test_size = 1.0 - train_size\n",
    "\n",
    "    if train_size is None:\n",
    "        train_size = 1.0 - test_size\n",
    "\n",
    "    if train_size + test_size > 1.0:\n",
    "        raise ValueError(\"train_size + test_size must be <= 1\")\n",
    "    \n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    N = len(X)   \n",
    "    X_train = X[:int(train_size*N)]\n",
    "    y_train = y[:int(train_size*N)]\n",
    "\n",
    "    X_test = X[int(train_size*N):]\n",
    "    y_test = y[int(train_size*N):]\n",
    "\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185656d5-3a58-4bd8-8b43-a81f498f1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 2)\n",
    "print(\"len(X_train) = \",len(X_train))\n",
    "print(\"len(y_train) = \",len(y_train))\n",
    "print(\"len(X_test) = \", len(X_test))\n",
    "print(\"len(y_test) = \", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb957e2-64a2-41bc-95b2-a3712d6570b4",
   "metadata": {},
   "source": [
    "<h1>Epoch Implementation (Training & Validation)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee10ed-ee06-45e8-81dd-bb74905999cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "model = Sequential([Linear(784, 256, weight_decay = 3e-04), \n",
    "                    ReLU(), \n",
    "                    Linear(256, 128, weight_decay = 3e-04), \n",
    "                    ReLU(), \n",
    "                    Linear(128, 10, weight_decay = 3e-04, weight_init = xavier_normal)])\n",
    "\n",
    "criterion = SoftmaxCrossEntropy()\n",
    "optimizer = Adam(model.layers, lr = 0.001, beta1 = 0.9, beta2 = 0.999)\n",
    "early_stopper = EarlyStopping(patience = 5)\n",
    "\n",
    "epoch_loss_train = []\n",
    "epoch_loss_test = []\n",
    "epoch_acc_train = []\n",
    "epoch_acc_test = []\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss_train_sum = 0.0\n",
    "    epoch_correct_train = 0\n",
    "    epoch_samples_train = 0\n",
    "    perm = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[perm]\n",
    "    y_train = y_train[perm]\n",
    "    \n",
    "    for i in tqdm(range(0, len(X_train), batch_size), desc = f\"Epoch: {epoch+1}/{epochs}\",leave = True):\n",
    "        X_train_batch = X_train[i : i + batch_size]\n",
    "        y_train_batch = y_train[i : i + batch_size]\n",
    "\n",
    "        \n",
    "        #training\n",
    "        \n",
    "        preds = model.forward(X_train_batch,training = True)\n",
    "        train_loss = criterion.forward(preds, y_train_batch)\n",
    "        \n",
    "        #L2 Regularization\n",
    "        l2_loss = compute_l2_loss(model)\n",
    "        train_loss += l2_loss\n",
    "        train_accuracy = accuracy_score(y_train_batch, preds)\n",
    "        \n",
    "        dout = criterion.backward()\n",
    "        model.backward(dout)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size = len(X_train_batch)\n",
    "        epoch_loss_train_sum += train_loss * batch_size\n",
    "        epoch_correct_train += np.sum(np.argmax(preds, axis = 1) == y_train_batch)\n",
    "        epoch_samples_train += batch_size\n",
    "\n",
    "    #Evaluation\n",
    "\n",
    "    preds_test = model.forward(X_test, training = False)\n",
    "    val_loss = criterion.forward(preds_test, y_test)\n",
    "    #L2 Regularization\n",
    "    l2_loss = compute_l2_loss(model)\n",
    "    val_loss += l2_loss\n",
    "    val_accuracy = accuracy_score(y_test, preds_test)\n",
    "    \n",
    "    epoch_loss_train.append(epoch_loss_train_sum/epoch_samples_train)\n",
    "    epoch_loss_test.append(val_loss)\n",
    "    epoch_acc_train.append(epoch_correct_train/epoch_samples_train)\n",
    "    epoch_acc_test.append(val_accuracy)\n",
    "    print(f\"train_loss: {epoch_loss_train[epoch]: .5f}, \"\n",
    "          f\"val_loss: {epoch_loss_test[epoch]: .5f}, \"\n",
    "          f\"train_accuracy: {epoch_acc_train[epoch]: .5f}, \"\n",
    "          f\"val_accuracy: {epoch_acc_test[epoch]: .5f}\")\n",
    "    \n",
    "    early_stopper.step(val_loss, model)\n",
    "    if early_stopper.stop :\n",
    "        print(f\"Early stopping executed at epoch: {epoch + 1}, \"\n",
    "              f\"best epoch: {early_stopper.best_epoch}, \"\n",
    "              f\"best_loss: {early_stopper.best_loss: .5f}\")\n",
    "        early_stopper.restore_best(model)\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time    \n",
    "print(f\"Total time required: {total_time :.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec58df",
   "metadata": {},
   "source": [
    "<h1>Training and Validation Metrics</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe14aaa-13ac-4967-8972-4cbbeb923f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, (len(epoch_loss_train) + 1))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, epoch_loss_train, label= \"Train loss\")\n",
    "plt.plot(epochs, epoch_loss_test, label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Validation Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc44e2-5a3d-4135-a07a-5a86838e663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(epochs, epoch_acc_train, label= \"Train Accuracy\")\n",
    "plt.plot(epochs, epoch_acc_test, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b47fab-4fc5-4c6f-9507-56c5bd0f333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 34587\n",
    "model.predict(X[i:i+1]), y[i:i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd2d71",
   "metadata": {},
   "source": [
    "<h1>Testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c19e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root=\"./data\", train = False, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for img, label in test_dataset :\n",
    "    X_list.append(img.numpy())\n",
    "    y_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_list)\n",
    "y_test = np.array(y_list)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.forward(X_test, training = False)\n",
    "test_loss = criterion.forward(preds_test, y_test)\n",
    "test_accuracy = accuracy_score(y_test, preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a7b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Loss: {test_loss: .5f}, Test accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MNIST Project)",
   "language": "python",
   "name": "mnist_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
